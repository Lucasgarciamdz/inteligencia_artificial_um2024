{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Trabajo Práctico Clasificación\n",
    "\n",
    "## Integrantes:\n",
    "\n",
    "* Augusto Kark\n",
    "* Lucas Garcia\n",
    "\n",
    "## Descripción del trabajo:\n",
    "\n",
    "> Consiste en el desarrollo de un modelo de clasificación para el diagnóstico de cáncer de seno maligno utilizando el dataset de sklearn (breast_cancer).\n",
    "\n",
    "Se implementara lo visto en clase paso por paso junto a una descripción detallada.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Importamos librerias y creamos el dataframe\n",
    "\n",
    "> Se agrega la columna 'diagnostico' que contiene el resultado del diagnóstico (0: maligno, 1: benigno)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "df = pd.DataFrame(data=data.data, columns=data.feature_names)\n",
    "\n",
    "df[\"diagnostico\"] = data.target"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Limpiamos y entendemos el dataset\n",
    "\n",
    "> El DataFrame contiene 569 entradas y 31 columnas. Cada columna representa una característica del conjunto de datos de cáncer de mama, con 30 columnas de tipo float64 y una columna de tipo int64 que representa el diagnóstico. No hay valores nulos en ninguna de las columnas. El conjunto de datos no contiene valores duplicados."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Entendemos el dataset\n",
    "df.info()\n",
    "df.describe()\n",
    "print(data.target_names)\n",
    "print(data.target)\n",
    "\n",
    "# Revisamos si hay valores duplicados\n",
    "print(f\"Cantidad de datos duplicados: {df.duplicated().sum()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Dividimos el dataset\n",
    "\n",
    "> Utilizamos StratifiedKFold para dividir el conjunto de datos en 5 pliegues, manteniendo la proporción de clases en cada grupo de entrenamiento y prueba. Escalamos los datos con StandardScaler para normalizarlos. Implementamos una función de visualización para mostrar la partición de los conjuntos de datos, utilizando matplotlib y ListedColormap para colorear las clases y los grupos de entrenamiento y prueba. Calculamos y mostramos la proporción de clases en cada pliegue de entrenamiento y prueba."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cmap_clases = ListedColormap(['#f21b3f', '#ff9914'])\n",
    "cmap_train_test = ListedColormap(['#8ac926', '#1982c4'])\n",
    "\n",
    "def visualize_split(classes, groups, splits):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    espaciado = list(range(splits, 0, -1))\n",
    "\n",
    "    # Graficar las clases\n",
    "    ax.scatter(range(len(classes)), [splits + 1.0] * len(classes), c=classes, marker='_', lw=25, cmap=cmap_clases)\n",
    "    legend1 = ax.legend([Patch(color=cmap_clases(0.0)), Patch(color=cmap_clases(1.0))], ['Clase 0', 'Clase 1'], loc='upper right')\n",
    "\n",
    "    if splits == 1:\n",
    "        ax.scatter(range(len(groups)), [splits] * len(groups), c=groups, marker='_', lw=25, cmap=cmap_train_test)\n",
    "        ax.set(yticks=[splits, splits + 1.0], yticklabels=['Entrenamiento/Testeo', 'Clases'], xlabel=\"Observaciones\")\n",
    "        ax.legend([Patch(color=cmap_train_test(0.0)), Patch(color=cmap_train_test(1.0))], ['Entrenamiento', 'Testeo'], loc='lower right')\n",
    "        ax.add_artist(legend1)\n",
    "\n",
    "    elif splits == len(classes):\n",
    "        for i in range(len(espaciado)):\n",
    "            ax.scatter(range(len(groups[i])), [espaciado[i]] * len(groups[i]), c=groups[i], marker='_', lw=25, cmap=cmap_train_test)\n",
    "        ax.legend([Patch(color=cmap_train_test(0.0)), Patch(color=cmap_train_test(1.0))], ['Entrenamiento', 'Testeo'], loc='lower right')\n",
    "        ax.add_artist(legend1)\n",
    "        if splits < 11:\n",
    "            ticklabels = [f'CV {x}' for x in reversed(espaciado)]\n",
    "            ticklabels.append('Clases')\n",
    "            espaciado.append(splits + 1)\n",
    "            ax.set(yticks=espaciado, yticklabels=ticklabels, xlabel=\"Observaciones\")\n",
    "\n",
    "    else:\n",
    "        for i in range(len(espaciado)):\n",
    "            ax.scatter(range(len(groups[i])), [espaciado[i]] * len(groups[i]), c=groups[i], marker='_', lw=25, cmap=cmap_train_test)\n",
    "        ax.legend([Patch(color=cmap_train_test(0.0)), Patch(color=cmap_train_test(1.0))], ['Entrenamiento', 'Testeo'], loc='lower right')\n",
    "        ax.add_artist(legend1)\n",
    "        if splits < 11:\n",
    "            ticklabels = [f'CV {x}' for x in reversed(espaciado)]\n",
    "            ticklabels.append('Clases')\n",
    "            espaciado.append(splits + 1)\n",
    "            ax.set(ylim=[0, splits + 2], yticks=espaciado, yticklabels=ticklabels, xlabel=\"Observaciones\")\n",
    "\n",
    "    plt.title('Visualización de la Partición de Conjuntos')\n",
    "    plt.show()\n",
    "\n",
    "X = df.drop(\"diagnostico\", axis=1)\n",
    "y = df[\"diagnostico\"]\n",
    "\n",
    "# Escalamos los datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "clases = y\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "grupos = []\n",
    "folds_indices = []\n",
    "\n",
    "# Datos de la partición\n",
    "nro = 1\n",
    "for train_index, test_index in skf.split(X_scaled, y):\n",
    "    grupo = np.zeros(y.shape)\n",
    "    for i in range(len(grupo)):\n",
    "        if i in train_index:\n",
    "            grupo[i] = 0\n",
    "        else:\n",
    "            grupo[i] = 1\n",
    "\n",
    "    grupos.append(grupo)\n",
    "    \n",
    "    folds_indices.append((train_index, test_index))\n",
    "\n",
    "    train_C0 = 0\n",
    "    train_C1 = 0\n",
    "    test_C0 = 0\n",
    "    test_C1 = 0\n",
    "\n",
    "    for j in range(len(grupo)):  # recorro la información de grupo de entrenamiento/test\n",
    "        if grupo[j] == 0 and clases[j] == 0:\n",
    "            train_C0 += 1\n",
    "        if grupo[j] == 0 and clases[j] == 1:\n",
    "            train_C1 += 1\n",
    "        if grupo[j] == 1 and clases[j] == 0:\n",
    "            test_C0 += 1\n",
    "        if grupo[j] == 1 and clases[j] == 1:\n",
    "            test_C1 += 1\n",
    "\n",
    "    print('CV %d: Entrenamiento %.2f C1/C0 | Test %.2f C1/C0' % (nro, train_C1 / train_C0, test_C1 / test_C0))\n",
    "    nro += 1\n",
    "\n",
    "visualize_split(clases, grupos, n_splits)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Ajuste\n",
    "\n",
    "> Aplicamos PCA para reducir los datos a 2 componentes principales y creamos un gráfico de dispersión (scatter plot) para visualizar la proyección PCA de las clases en el conjunto de datos de cáncer de mama, diferenciando entre las clases malignas y benignas con diferentes colores. Se logra ver una separación lineal entre las clases malignas y benignas por lo que utilizamos un Análisis Discriminante Lineal para el ajuste."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Aplicar PCA para reducir los datos a 2 componentes principales\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Crear un gráfico de dispersión (scatter plot)\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red', 'blue']\n",
    "labels = ['Malignant', 'Benign']\n",
    "\n",
    "for i in range(2):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], c=colors[i], label=labels[i], alpha=0.5)\n",
    "\n",
    "# Añadir etiquetas y leyenda\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.title('Proyección PCA de las clases en el conjunto de datos de cáncer de mama')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Definir el modelo LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Lista para almacenar los modelos entrenados\n",
    "trained_models = []\n",
    "\n",
    "# Entrenar el modelo en cada fold\n",
    "for train_index, test_index in folds_indices:\n",
    "    # Dividir los datos en entrenamiento y prueba usando los índices generados\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Entrenar el modelo LDA\n",
    "    lda.fit(X_train, y_train)\n",
    "\n",
    "    # Almacenar el modelo entrenado\n",
    "    trained_models.append((lda, test_index))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "##  Visualización de fronteras de decisión y métricas\n",
    "\n",
    "> Evaluamos el rendimiento del modelo LDA en cada conjunto de prueba y calculamos las métricas de precisión, precisión, recall y F1-score. Además, visualizamos las fronteras de decisión utilizando PCA para reducir los datos a 2 componentes principales y creamos gráficos de dispersión para cada conjunto de prueba. Finalmente, mostramos las métricas promedio y la matriz de confusión acumulada."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style for prettier plots\n",
    "plt.style.use('seaborn-v0_8-pastel')\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "# Lista para almacenar las métricas\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "# Evaluar cada modelo en sus respectivos conjuntos de prueba\n",
    "for i, (model, test_index) in enumerate(trained_models):\n",
    "    # Obtener los datos de prueba correspondientes\n",
    "    X_test = X_scaled[test_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    # Hacer predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calcular las métricas y almacenarlas\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred))\n",
    "    recall_scores.append(recall_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    confusion_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Visualización de fronteras de decisión\n",
    "    pca = PCA(n_components=2)\n",
    "    X_test_pca = pca.fit_transform(X_test)\n",
    "    xx, yy = np.meshgrid(np.linspace(X_test_pca[:, 0].min(), X_test_pca[:, 0].max(), 1000),\n",
    "                         np.linspace(X_test_pca[:, 1].min(), X_test_pca[:, 1].max(), 1000))\n",
    "    Z = model.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap='RdYlBu')\n",
    "    scatter = plt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, edgecolor='black', cmap='RdYlBu', s=50)\n",
    "    plt.xlabel('Principal Component 1', fontsize=12)\n",
    "    plt.ylabel('Principal Component 2', fontsize=12)\n",
    "    plt.title('LDA Decision Boundary (Breast Cancer Dataset)', fontsize=16)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Visualización 3D de LDA\n",
    "    xx, yy = np.meshgrid(np.linspace(X_test_pca[:, 0].min(), X_test_pca[:, 0].max(), 100),\n",
    "                         np.linspace(X_test_pca[:, 1].min(), X_test_pca[:, 1].max(), 100))\n",
    "\n",
    "    Z = model.predict_proba(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # 3D Plot with improvements\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Smoother surface, improved colormap, and transparency\n",
    "    surf = ax.plot_surface(xx, yy, Z, cmap='coolwarm', alpha=0.7, antialiased=True, rstride=1, cstride=1,\n",
    "                           edgecolor='none', shade=True)\n",
    "\n",
    "    # Scatter plot for data points with better edge contrast\n",
    "    scatter = ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], y_test, c=y_test, cmap='coolwarm',\n",
    "                         edgecolor='black', s=60, alpha=1)\n",
    "\n",
    "    # Axis labels\n",
    "    ax.set_xlabel('Principal Component 1', fontsize=12)\n",
    "    ax.set_ylabel('Principal Component 2', fontsize=12)\n",
    "    ax.set_zlabel('Probability', fontsize=12)\n",
    "    ax.set_title('3D Visualization of LDA', fontsize=16)\n",
    "\n",
    "    # Improved viewpoint (adjust `elev` and `azim` as needed)\n",
    "    ax.view_init(elev=30, azim=240)\n",
    "\n",
    "    # Color bar for surface plot\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Mostrar las métricas\n",
    "print(\"\\nMétricas de rendimiento del modelo LDA:\")\n",
    "print(f'Precisión media: {np.mean(accuracy_scores):.4f} (±{np.std(accuracy_scores):.4f})')\n",
    "print(f'Precisión media: {np.mean(precision_scores):.4f} (±{np.std(precision_scores):.4f})')\n",
    "print(f'Recall medio: {np.mean(recall_scores):.4f} (±{np.std(recall_scores):.4f})')\n",
    "print(f'F1-score medio: {np.mean(f1_scores):.4f} (±{np.std(f1_scores):.4f})')\n",
    "\n",
    "# Visualizar la matriz de confusión acumulada\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.heatmap(np.sum(confusion_matrices, axis=0), annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Accumulated Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Ajuste de hiperparámetros\n",
    "\n",
    "> En esta sección, realizamos un ajuste de hiperparámetros para el modelo de Análisis Discriminante Lineal (LDA) utilizando RandomizedSearchCV. Se probaron diferentes combinaciones de hiperparámetros para encontrar la configuración óptima que maximice la precisión del modelo. A continuación, se presentan las estadísticas de los parámetros obtenidos y los mejores parámetros globales seleccionados:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Definir los hiperparámetros a probar\n",
    "param_distributions_list = [\n",
    "    {\n",
    "        'solver': ['eigen', 'lsqr'],\n",
    "        'shrinkage': uniform(0, 1),\n",
    "        'n_components': randint(1, min(len(np.unique(y)) - 1, X_scaled.shape[1]) + 1),\n",
    "        'priors': [None],\n",
    "        'store_covariance': [False, True],\n",
    "        'covariance_estimator': [None]\n",
    "    },\n",
    "    {\n",
    "        'solver': ['svd'],\n",
    "        'tol': uniform(1e-5, 1e-3),\n",
    "        'n_components': randint(1, min(len(np.unique(y)) - 1, X_scaled.shape[1]) + 1),\n",
    "        'priors': [None],\n",
    "        'store_covariance': [False, True]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Diccionario para almacenar todos los valores de parámetros\n",
    "all_params = defaultdict(list)\n",
    "\n",
    "# Entrenar el modelo en cada fold\n",
    "for train_index, test_index in folds_indices:\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    for param_distributions in param_distributions_list:\n",
    "        random_search = RandomizedSearchCV(\n",
    "            LinearDiscriminantAnalysis(),\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=100,\n",
    "            cv=5,\n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        # Almacenar los mejores parámetros\n",
    "        for key, value in random_search.best_params_.items():\n",
    "            all_params[key].append(value)\n",
    "\n",
    "# Calcular y mostrar estadísticas de los parámetros\n",
    "print(\"\\nEstadísticas de los parámetros:\")\n",
    "for key, values in all_params.items():\n",
    "    if isinstance(values[0], (int, float)):\n",
    "        mean_value = np.mean(values)\n",
    "        median_value = np.median(values)\n",
    "        std_value = np.std(values)\n",
    "        print(f\"{key}:\")\n",
    "        print(f\"  Media: {mean_value:.6f}\")\n",
    "        print(f\"  Mediana: {median_value:.6f}\")\n",
    "        print(f\"  Desviación estándar: {std_value:.6f}\")\n",
    "    else:\n",
    "        value_counts = defaultdict(int)\n",
    "        for value in values:\n",
    "            value_counts[value] += 1\n",
    "        most_common = max(value_counts, key=value_counts.get)\n",
    "        print(f\"{key}:\")\n",
    "        print(f\"  Valor más común: {most_common}\")\n",
    "        print(f\"  Distribución: {dict(value_counts)}\")\n",
    "    print()\n",
    "\n",
    "# Calcular los mejores parámetros globales\n",
    "best_params = {}\n",
    "for key, values in all_params.items():\n",
    "    if isinstance(values[0], (int, float)):\n",
    "        best_params[key] = np.median(values)\n",
    "    else:\n",
    "        best_params[key] = max(set(values), key=values.count)\n",
    "\n",
    "print(\"Mejores parámetros globales:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"{key}: {value}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Nuevos Parámetros\n",
    "\n",
    "> Utilizamos los mejores parámetros obtenidos durante el ajuste de hiperparámetros para entrenar un nuevo modelo de Análisis Discriminante Lineal (LDA) y compararlo con el modelo anterior. Notamos que no hay una mejora significativa."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Set the style for prettier plots\n",
    "plt.style.use('seaborn-v0_8-pastel')\n",
    "sns.set_palette(\"deep\")\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(trained_models, X_scaled, y):\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    confusion_matrices = []\n",
    "\n",
    "    for i, (model, test_index) in enumerate(trained_models):\n",
    "        X_test = X_scaled[test_index]\n",
    "        y_test = y[test_index]\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "        precision_scores.append(precision_score(y_test, y_pred))\n",
    "        recall_scores.append(recall_score(y_test, y_pred))\n",
    "        f1_scores.append(f1_score(y_test, y_pred))\n",
    "        confusion_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    print(\"\\nMétricas de rendimiento del modelo LDA:\")\n",
    "    print(f'Precisión media: {np.mean(accuracy_scores):.4f} (±{np.std(accuracy_scores):.4f})')\n",
    "    print(f'Precisión media: {np.mean(precision_scores):.4f} (±{np.std(precision_scores):.4f})')\n",
    "    print(f'Recall medio: {np.mean(recall_scores):.4f} (±{np.std(recall_scores):.4f})')\n",
    "    print(f'F1-score medio: {np.mean(f1_scores):.4f} (±{np.std(f1_scores):.4f})')\n",
    "\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    sns.heatmap(np.sum(confusion_matrices, axis=0), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Accumulated Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Train and evaluate the model without hyperparameter optimization\n",
    "lda_default = LinearDiscriminantAnalysis()\n",
    "trained_models_default = []\n",
    "\n",
    "for train_index, test_index in folds_indices:\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lda_default.fit(X_train, y_train)\n",
    "    trained_models_default.append((lda_default, test_index))\n",
    "\n",
    "print(\"Evaluación sin optimización de hiperparámetros:\")\n",
    "evaluate_model(trained_models_default, X_scaled, y)\n",
    "\n",
    "# Train and evaluate the model with hyperparameter optimization\n",
    "lda_optimized = LinearDiscriminantAnalysis(\n",
    "    solver=best_params.get('solver'),\n",
    "    n_components=int(best_params.get('n_components')),\n",
    "    priors=best_params.get('priors'),\n",
    "    store_covariance=bool(best_params.get('store_covariance')),\n",
    "    covariance_estimator=best_params.get('covariance_estimator')\n",
    ")\n",
    "trained_models_optimized = []\n",
    "\n",
    "for train_index, test_index in folds_indices:\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    lda_optimized.fit(X_train, y_train)\n",
    "    trained_models_optimized.append((lda_optimized, test_index))\n",
    "\n",
    "print(\"Evaluación con optimización de hiperparámetros:\")\n",
    "evaluate_model(trained_models_optimized, X_scaled, y)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
